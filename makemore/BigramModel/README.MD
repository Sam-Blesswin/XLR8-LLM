# ğŸ“˜ Bigram Name Generator

This notebook walks through building a simple **character-level language model** to generate human-like names. It has two parts:

1. **Manual Bigram Counting** â€” a no-learning, statistics-based approach.
2. **Neural Network** â€” a tiny learning model that figures out bigrams on its own.

Both approaches lead to surprisingly similar results, illustrating the deep connection between counting and learning.

---

## ğŸ§© Part 1: Manual Bigram Counting

### ğŸ§  What We Do

We read a dataset of names and break them down into **bigrams** â€” pairs of consecutive characters (e.g., `'e' â†’ 'm'` in "emma"). We add special `.` tokens to mark the beginning and end of each name.

For example, `"emma"` becomes:

* `.` â†’ `e`
* `e` â†’ `m`
* `m` â†’ `m`
* `m` â†’ `a`
* `a` â†’ `.`

We count all such pairs across all names.

### Bigram Counting

* A bigram is a pair of consecutive characters (like 'a' followed by 'n').
* The model first counts how often each character is followed by another across all names. This gives us an idea of transition frequencies between characters.

### ğŸ“Š What We Get

* A matrix of counts showing how often each character follows another.
* These counts are normalized to create **probabilities** of transitions.
* We use these to randomly generate new names by sampling character-by-character.

### âœ… Why It Works

Itâ€™s direct, interpretable, and needs no training â€” just counting and normalizing. Itâ€™s a great baseline.

---

## ğŸ§  Part 2: Neural Network

### ğŸ” What We Build

We construct a tiny neural network that learns the same bigram relationships â€” but instead of counting explicitly, it **learns a weight matrix** that maps each input character to a probability distribution over the next character.

Steps:

1. Convert each character to a **one-hot vector**.
2. Multiply by a **weight matrix** to get a vector of scores (logits).
3. Apply **softmax** to get probabilities.
4. Use **negative log-likelihood loss** to evaluate predictions.
5. Update the weights using **gradient descent**.

After enough training, the network generates names almost indistinguishable from those in Part 1.

---

## ğŸ” Comparison: Manual vs Neural

Even though one method just counts and the other learns, theyâ€™re mathematically close:

| Concept        | Part 1: Manual Counting | Part 2: Neural Network      |
| -------------- | ----------------------- | --------------------------- |
| Basis          | Raw bigram counts       | Learned weights (logits)    |
| Probabilities  | Normalized counts       | Softmax of logits           |
| Training       | None                    | Gradient descent on loss    |
| Output quality | Surprisingly good       | Very similar                |
| Bonus trick    | `exp(logits)` â‰ˆ counts  | Shows they're deeply linked |

### ğŸ¤¯ Insight

The neural network doesnâ€™t learn something magical â€” it learns **something equivalent to counting bigrams**, just encoded differently (log space). When you `exp(logits)`, the results align closely with the actual counts from Part 1.

---

## ğŸ§  Key Concepts

* **Bigram**: Pair of consecutive characters.
* **One-Hot Encoding**: A binary vector to represent each character.
* **Softmax**: Converts logits to probabilities.
* **Negative Log-Likelihood**: Measures how "wrong" the model is.

---
